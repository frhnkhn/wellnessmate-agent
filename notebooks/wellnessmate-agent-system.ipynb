{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# WellnessMate: Multi-Agent Health & Wellness Assistant\n\nThis project is my capstone submission for the Kaggle x Google AI Agents Intensive.","metadata":{}},{"cell_type":"markdown","source":"## Key AI Agents Concepts Demonstrated\n\nIn this capstone project, I apply multiple core concepts from the **Kaggle x Google AI Agents Intensive**. The project demonstrates at least **five** of the required agentic capabilities:\n\n---\n\n### **1. Multi-Agent System (Sequential + Loop Behaviour)**  \nThis project uses a structured multi-agent architecture:\n\n- **PlannerAgent** – understands the user’s query and creates an analysis plan.\n- **AnalyticsAgent** – runs the actual data analysis using tools (statistics, trends, correlations).\n- **CoachAgent** – turns numerical results into personalised wellness recommendations.\n\nThe agents run in a **sequential pipeline**:\n\n> User → PlannerAgent → AnalyticsAgent → CoachAgent → Final Output\n\nI also show a simple **loop-style behaviour**, where the system can run multiple iterations to update or refine recommendations with new data.\n\n---\n\n### **2. Tools (Custom Tools + Code Execution)**  \nThe agents do not directly manipulate the raw data.  \nInstead, they call a registry of **custom tools**, such as:\n\n- `filter_by_date()`\n- `compute_basic_stats()`\n- `compute_correlations()`\n- `recent_trend()`\n- (optional) `generate_plot()` using Python execution\n\nThis follows the tool-calling pattern taught in the course, where agents “take action” by calling tools rather than embedding all logic internally.\n\n---\n\n### **3. Sessions & Memory (Short-Term + Long-Term)**  \nThis system uses **two layers of memory**:\n\n- **Session memory**  \n  Stores the current conversation state and intermediate results.\n\n- **Long-term memory**  \n  Implemented as a simple JSON store that saves recurring wellness insights, such as patterns between sleep and mood.\n\nThis demonstrates how agents can use both **short-term session context** and **persistent long-term knowledge**.\n\n---\n\n### **4. Observability (Logging, Tracing, Metrics)**  \nThe system logs every major action into a shared **trace**, including:\n\n- which agent ran,\n- which tools were used,\n- timestamps for each step.\n\nI also track simple **metrics**:\n\n- number of tool calls,\n- runtime,\n- number of generated recommendations.\n\nThis reflects the observability practices taught in the course (logging, tracing, metrics).\n\n---\n\n### **5. Agent Evaluation**  \nI include lightweight evaluation checks:\n\n- verifying dataset loading,\n- checking that statistical tools produce expected outputs,\n- ensuring that at least one recommendation is generated for normal queries.\n\nThis shows a basic but clear approach to **agent quality evaluation**, as required in the capstone.\n","metadata":{}},{"cell_type":"markdown","source":"## System Architecture Overview\n\n### **1. High-Level Concept**\nThe idea behind WellnessMate is simple:\n\n> “Given a user’s daily wellness logs (sleep, steps, mood, hydration, workouts),  \n> how can an AI agent system understand the user’s questions, analyze their wellness patterns,  \n> and suggest small, personalised improvements?”\n\nRather than using a single large script, the project uses multiple collaborating **agents**, each responsible for part of the workflow.\n\n---\n\n### **2. Agents in the System**\n\n#### **1. PlannerAgent**\n- Reads the user’s natural language question.  \n- Determines what type of analysis is needed (trends, correlations, general overview, etc.).  \n- Produces a simple **plan** describing which steps and tools should run.\n\n#### **2. AnalyticsAgent**\n- Takes the plan from the PlannerAgent.  \n- Uses tools to analyze the wellness dataset:\n  - compute basic statistics,\n  - extract recent trends,\n  - compute correlations between variables.\n\n#### **3. CoachAgent**\n- Takes the analysis results.  \n- Converts numerical data into practical, personalised wellness recommendations.  \n- Stores useful insights into **long-term memory** (Memory Bank style).\n\nTogether, these agents work in a **sequential pipeline**:\n> Planner → Analytics → Coach\n\n---\n\n### **3. Tools Layer**\nTo follow the agentic pattern from the course, the system uses **tools** instead of embedding logic directly in agents.\n\nExamples of custom tools:\n- `compute_basic_stats()` – mean/min/max for each metric  \n- `compute_correlations()` – checks relationships (e.g., sleep ↔ mood)  \n- `recent_trend()` – extracts the last 7/14/30 days of data  \n- `filter_by_date()` – optional date filtering  \n- `generate_plot()` – code-based tool to create charts\n\nAgents call these tools through a `tool_registry`, which imitates an MCP-style tool discovery mechanism.\n\n---\n\n### **4. Memory Layer**\n\n#### **Session Memory**\n- Tracks the current conversation.\n- Stores partial results within a single agent run.\n\n#### **Long-Term Memory**\n- A simple JSON file (`wellness_memory.json`).\n- Stores repeated or meaningful insights so the agent can re-use them across sessions.\n- Demonstrates the concept of a lightweight **Memory Bank**.\n\n---\n\n### **5. Observability & Evaluation**\n\nThe system includes:\n\n- **Logging & Tracing:**  \n  Every agent action is recorded in a trace list with timestamps.\n\n- **Metrics:**  \n  For each run, the system records:\n  - tool calls count,\n  - runtime,\n  - number of recommendations generated.\n\n- **Evaluation Checks:**  \n  Small tests verify:\n  - the dataset loads,\n  - the tools return expected outputs,\n  - the agent produces suggestions.\n\nThese elements support debugging, transparency, and quality control, aligning with the course’s best practices for **agent observability**.\n\n---\n\n### **6. End-to-End Flow**\n\n> User query  \n> → PlannerAgent creates a plan  \n> → AnalyticsAgent executes tools  \n> → CoachAgent generates personalised recommendations  \n> → Logs, memory updates, and metrics recorded  \n> → Final wellness insights returned\n\nThis complete flow showcases how an AI agentic system can analyze wellness data and provide user-friendly, actionable insights.\n","metadata":{}}]}